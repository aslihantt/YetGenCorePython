{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5b519bf4",
      "metadata": {
        "id": "5b519bf4"
      },
      "source": [
        "Before running, install required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c55af073",
      "metadata": {
        "id": "c55af073",
        "outputId": "dc161f90-58ac-4776-ce64-64fab50b7447",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.11/dist-packages (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytorch-ignite) (24.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install numpy torch torchvision pytorch-ignite"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e87b8f3a",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "e87b8f3a"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "30000828",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "30000828"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import models, datasets, transforms\n",
        "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
        "from ignite.metrics import Accuracy, Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1453f53e",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "1453f53e"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db9e1a8a",
      "metadata": {
        "id": "db9e1a8a"
      },
      "source": [
        "Dataset MNIST will be loaded further down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3871294f",
      "metadata": {
        "id": "3871294f"
      },
      "outputs": [],
      "source": [
        "# Set up hyperparameters.\n",
        "lr = 0.001\n",
        "batch_size = 128\n",
        "num_epochs = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "15ddc0ea",
      "metadata": {
        "id": "15ddc0ea"
      },
      "outputs": [],
      "source": [
        "# Set up logging.\n",
        "print_every = 1  # batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1c67ecab",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "1c67ecab"
      },
      "outputs": [],
      "source": [
        "# Set up device.\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "481fa930",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "481fa930"
      },
      "source": [
        "# Dataset & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9bb00cee",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "9bb00cee"
      },
      "outputs": [],
      "source": [
        "def load_data(train):\n",
        "    # Download and transform dataset.\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # grayscale to RGB\n",
        "    ])\n",
        "    dataset = datasets.MNIST(\"./data\", train=train, download=True, transform=transform)\n",
        "\n",
        "    # Wrap in data loader.\n",
        "    if use_cuda:\n",
        "        kwargs = {\"pin_memory\": True, \"num_workers\": 1}\n",
        "    else:\n",
        "        kwargs = {}\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=train, **kwargs)\n",
        "    return loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a64cad4d",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "a64cad4d",
        "outputId": "4a2073cc-6309-4ac0-efb8-74bc562e1ebc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 21.7MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 645kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.86MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.37MB/s]\n"
          ]
        }
      ],
      "source": [
        "train_loader = load_data(train=True)\n",
        "val_loader = None\n",
        "test_loader = load_data(train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e02648d9",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "e02648d9"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8cf788bc",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "8cf788bc",
        "outputId": "8feb8c1b-6290-4748-b375-968424f664cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Set up model, loss, optimizer.\n",
        "model = models.alexnet(pretrained=False)\n",
        "model = model.to(device)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ebbb605",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "6ebbb605"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9ac7ed85",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "9ac7ed85"
      },
      "outputs": [],
      "source": [
        "# Set up pytorch-ignite trainer and evaluator.\n",
        "trainer = create_supervised_trainer(\n",
        "    model,\n",
        "    optimizer,\n",
        "    loss_func,\n",
        "    device=device,\n",
        ")\n",
        "metrics = {\n",
        "    \"accuracy\": Accuracy(),\n",
        "    \"loss\": Loss(loss_func),\n",
        "}\n",
        "evaluator = create_supervised_evaluator(\n",
        "    model, metrics=metrics, device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7a3ad04b",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "7a3ad04b"
      },
      "outputs": [],
      "source": [
        "@trainer.on(Events.ITERATION_COMPLETED(every=print_every))\n",
        "def log_batch(trainer):\n",
        "    batch = (trainer.state.iteration - 1) % trainer.state.epoch_length + 1\n",
        "    print(\n",
        "        f\"Epoch {trainer.state.epoch} / {num_epochs}, \"\n",
        "        f\"batch {batch} / {trainer.state.epoch_length}: \"\n",
        "        f\"loss: {trainer.state.output:.3f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e34c51f7",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "e34c51f7"
      },
      "outputs": [],
      "source": [
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def log_epoch(trainer):\n",
        "    print(f\"Epoch {trainer.state.epoch} / {num_epochs} average results: \")\n",
        "\n",
        "    def log_results(name, metrics, epoch):\n",
        "        print(\n",
        "            f\"{name + ':':6} loss: {metrics['loss']:.3f}, \"\n",
        "            f\"accuracy: {metrics['accuracy']:.3f}\"\n",
        "        )\n",
        "\n",
        "    # Train data.\n",
        "    evaluator.run(train_loader)\n",
        "    log_results(\"train\", evaluator.state.metrics, trainer.state.epoch)\n",
        "\n",
        "    # Val data.\n",
        "    if val_loader:\n",
        "        evaluator.run(val_loader)\n",
        "        log_results(\"val\", evaluator.state.metrics, trainer.state.epoch)\n",
        "\n",
        "    # Test data.\n",
        "    if test_loader:\n",
        "        evaluator.run(test_loader)\n",
        "        log_results(\"test\", evaluator.state.metrics, trainer.state.epoch)\n",
        "\n",
        "    print()\n",
        "    print(\"-\" * 80)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c11b1f2e",
      "metadata": {
        "id": "c11b1f2e",
        "outputId": "feafe82c-bc6a-47b9-9f26-63b9a9d999de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 3, batch 1 / 469: loss: 6.906\n",
            "Epoch 1 / 3, batch 2 / 469: loss: 3.103\n",
            "Epoch 1 / 3, batch 3 / 469: loss: 144.111\n",
            "Epoch 1 / 3, batch 4 / 469: loss: 10.242\n",
            "Epoch 1 / 3, batch 5 / 469: loss: 4.851\n",
            "Epoch 1 / 3, batch 6 / 469: loss: 6.040\n",
            "Epoch 1 / 3, batch 7 / 469: loss: 5.752\n",
            "Epoch 1 / 3, batch 8 / 469: loss: 4.952\n",
            "Epoch 1 / 3, batch 9 / 469: loss: 4.026\n",
            "Epoch 1 / 3, batch 10 / 469: loss: 3.494\n",
            "Epoch 1 / 3, batch 11 / 469: loss: 2.796\n",
            "Epoch 1 / 3, batch 12 / 469: loss: 3.784\n",
            "Epoch 1 / 3, batch 13 / 469: loss: 3.929\n",
            "Epoch 1 / 3, batch 14 / 469: loss: 4.265\n",
            "Epoch 1 / 3, batch 15 / 469: loss: 3.481\n",
            "Epoch 1 / 3, batch 16 / 469: loss: 3.072\n",
            "Epoch 1 / 3, batch 17 / 469: loss: 3.290\n",
            "Epoch 1 / 3, batch 18 / 469: loss: 3.420\n",
            "Epoch 1 / 3, batch 19 / 469: loss: 2.909\n",
            "Epoch 1 / 3, batch 20 / 469: loss: 2.879\n",
            "Epoch 1 / 3, batch 21 / 469: loss: 2.937\n",
            "Epoch 1 / 3, batch 22 / 469: loss: 2.914\n",
            "Epoch 1 / 3, batch 23 / 469: loss: 2.823\n",
            "Epoch 1 / 3, batch 24 / 469: loss: 2.618\n",
            "Epoch 1 / 3, batch 25 / 469: loss: 2.941\n",
            "Epoch 1 / 3, batch 26 / 469: loss: 3.079\n",
            "Epoch 1 / 3, batch 27 / 469: loss: 2.775\n",
            "Epoch 1 / 3, batch 28 / 469: loss: 2.798\n",
            "Epoch 1 / 3, batch 29 / 469: loss: 2.758\n",
            "Epoch 1 / 3, batch 30 / 469: loss: 2.537\n",
            "Epoch 1 / 3, batch 31 / 469: loss: 2.672\n",
            "Epoch 1 / 3, batch 32 / 469: loss: 2.682\n",
            "Epoch 1 / 3, batch 33 / 469: loss: 2.842\n",
            "Epoch 1 / 3, batch 34 / 469: loss: 2.602\n",
            "Epoch 1 / 3, batch 35 / 469: loss: 2.601\n",
            "Epoch 1 / 3, batch 36 / 469: loss: 2.670\n",
            "Epoch 1 / 3, batch 37 / 469: loss: 2.505\n",
            "Epoch 1 / 3, batch 38 / 469: loss: 2.478\n",
            "Epoch 1 / 3, batch 39 / 469: loss: 2.475\n",
            "Epoch 1 / 3, batch 40 / 469: loss: 2.525\n",
            "Epoch 1 / 3, batch 41 / 469: loss: 2.480\n",
            "Epoch 1 / 3, batch 42 / 469: loss: 2.459\n",
            "Epoch 1 / 3, batch 43 / 469: loss: 2.479\n",
            "Epoch 1 / 3, batch 44 / 469: loss: 2.412\n",
            "Epoch 1 / 3, batch 45 / 469: loss: 2.377\n",
            "Epoch 1 / 3, batch 46 / 469: loss: 2.404\n",
            "Epoch 1 / 3, batch 47 / 469: loss: 2.373\n",
            "Epoch 1 / 3, batch 48 / 469: loss: 2.324\n",
            "Epoch 1 / 3, batch 49 / 469: loss: 2.319\n",
            "Epoch 1 / 3, batch 50 / 469: loss: 2.335\n",
            "Epoch 1 / 3, batch 51 / 469: loss: 2.316\n",
            "Epoch 1 / 3, batch 52 / 469: loss: 2.305\n",
            "Epoch 1 / 3, batch 53 / 469: loss: 2.326\n",
            "Epoch 1 / 3, batch 54 / 469: loss: 2.356\n",
            "Epoch 1 / 3, batch 55 / 469: loss: 2.351\n",
            "Epoch 1 / 3, batch 56 / 469: loss: 2.332\n",
            "Epoch 1 / 3, batch 57 / 469: loss: 2.335\n",
            "Epoch 1 / 3, batch 58 / 469: loss: 2.306\n",
            "Epoch 1 / 3, batch 59 / 469: loss: 2.312\n",
            "Epoch 1 / 3, batch 60 / 469: loss: 2.305\n",
            "Epoch 1 / 3, batch 61 / 469: loss: 2.326\n",
            "Epoch 1 / 3, batch 62 / 469: loss: 2.320\n",
            "Epoch 1 / 3, batch 63 / 469: loss: 2.339\n",
            "Epoch 1 / 3, batch 64 / 469: loss: 2.377\n",
            "Epoch 1 / 3, batch 65 / 469: loss: 2.352\n",
            "Epoch 1 / 3, batch 66 / 469: loss: 2.301\n",
            "Epoch 1 / 3, batch 67 / 469: loss: 2.354\n",
            "Epoch 1 / 3, batch 68 / 469: loss: 2.321\n",
            "Epoch 1 / 3, batch 69 / 469: loss: 2.325\n",
            "Epoch 1 / 3, batch 70 / 469: loss: 2.284\n",
            "Epoch 1 / 3, batch 71 / 469: loss: 2.314\n",
            "Epoch 1 / 3, batch 72 / 469: loss: 2.300\n",
            "Epoch 1 / 3, batch 73 / 469: loss: 2.351\n",
            "Epoch 1 / 3, batch 74 / 469: loss: 2.349\n",
            "Epoch 1 / 3, batch 75 / 469: loss: 2.319\n",
            "Epoch 1 / 3, batch 76 / 469: loss: 2.376\n",
            "Epoch 1 / 3, batch 77 / 469: loss: 2.304\n",
            "Epoch 1 / 3, batch 78 / 469: loss: 2.325\n",
            "Epoch 1 / 3, batch 79 / 469: loss: 2.305\n",
            "Epoch 1 / 3, batch 80 / 469: loss: 2.321\n",
            "Epoch 1 / 3, batch 81 / 469: loss: 2.310\n",
            "Epoch 1 / 3, batch 82 / 469: loss: 2.311\n",
            "Epoch 1 / 3, batch 83 / 469: loss: 2.321\n",
            "Epoch 1 / 3, batch 84 / 469: loss: 2.320\n",
            "Epoch 1 / 3, batch 85 / 469: loss: 2.291\n",
            "Epoch 1 / 3, batch 86 / 469: loss: 2.327\n",
            "Epoch 1 / 3, batch 87 / 469: loss: 2.342\n",
            "Epoch 1 / 3, batch 88 / 469: loss: 2.293\n",
            "Epoch 1 / 3, batch 89 / 469: loss: 2.334\n",
            "Epoch 1 / 3, batch 90 / 469: loss: 2.304\n",
            "Epoch 1 / 3, batch 91 / 469: loss: 2.316\n",
            "Epoch 1 / 3, batch 92 / 469: loss: 2.312\n",
            "Epoch 1 / 3, batch 93 / 469: loss: 2.278\n",
            "Epoch 1 / 3, batch 94 / 469: loss: 2.315\n",
            "Epoch 1 / 3, batch 95 / 469: loss: 2.348\n",
            "Epoch 1 / 3, batch 96 / 469: loss: 2.309\n",
            "Epoch 1 / 3, batch 97 / 469: loss: 2.303\n",
            "Epoch 1 / 3, batch 98 / 469: loss: 2.323\n",
            "Epoch 1 / 3, batch 99 / 469: loss: 2.319\n",
            "Epoch 1 / 3, batch 100 / 469: loss: 2.318\n",
            "Epoch 1 / 3, batch 101 / 469: loss: 2.303\n",
            "Epoch 1 / 3, batch 102 / 469: loss: 2.308\n",
            "Epoch 1 / 3, batch 103 / 469: loss: 2.286\n",
            "Epoch 1 / 3, batch 104 / 469: loss: 2.316\n",
            "Epoch 1 / 3, batch 105 / 469: loss: 2.308\n",
            "Epoch 1 / 3, batch 106 / 469: loss: 2.310\n",
            "Epoch 1 / 3, batch 107 / 469: loss: 2.312\n",
            "Epoch 1 / 3, batch 108 / 469: loss: 2.298\n",
            "Epoch 1 / 3, batch 109 / 469: loss: 2.316\n",
            "Epoch 1 / 3, batch 110 / 469: loss: 2.313\n",
            "Epoch 1 / 3, batch 111 / 469: loss: 2.316\n",
            "Epoch 1 / 3, batch 112 / 469: loss: 2.308\n",
            "Epoch 1 / 3, batch 113 / 469: loss: 2.299\n",
            "Epoch 1 / 3, batch 114 / 469: loss: 2.327\n",
            "Epoch 1 / 3, batch 115 / 469: loss: 2.301\n",
            "Epoch 1 / 3, batch 116 / 469: loss: 2.297\n",
            "Epoch 1 / 3, batch 117 / 469: loss: 2.303\n",
            "Epoch 1 / 3, batch 118 / 469: loss: 2.297\n",
            "Epoch 1 / 3, batch 119 / 469: loss: 2.332\n",
            "Epoch 1 / 3, batch 120 / 469: loss: 2.316\n",
            "Epoch 1 / 3, batch 121 / 469: loss: 2.320\n",
            "Epoch 1 / 3, batch 122 / 469: loss: 2.305\n",
            "Epoch 1 / 3, batch 123 / 469: loss: 2.339\n",
            "Epoch 1 / 3, batch 124 / 469: loss: 2.313\n",
            "Epoch 1 / 3, batch 125 / 469: loss: 2.332\n",
            "Epoch 1 / 3, batch 126 / 469: loss: 2.308\n",
            "Epoch 1 / 3, batch 127 / 469: loss: 2.318\n",
            "Epoch 1 / 3, batch 128 / 469: loss: 2.311\n",
            "Epoch 1 / 3, batch 129 / 469: loss: 2.302\n",
            "Epoch 1 / 3, batch 130 / 469: loss: 2.301\n",
            "Epoch 1 / 3, batch 131 / 469: loss: 2.311\n",
            "Epoch 1 / 3, batch 132 / 469: loss: 2.300\n",
            "Epoch 1 / 3, batch 133 / 469: loss: 2.303\n",
            "Epoch 1 / 3, batch 134 / 469: loss: 2.300\n",
            "Epoch 1 / 3, batch 135 / 469: loss: 2.316\n",
            "Epoch 1 / 3, batch 136 / 469: loss: 2.286\n",
            "Epoch 1 / 3, batch 137 / 469: loss: 2.307\n",
            "Epoch 1 / 3, batch 138 / 469: loss: 2.308\n",
            "Epoch 1 / 3, batch 139 / 469: loss: 2.314\n",
            "Epoch 1 / 3, batch 140 / 469: loss: 2.330\n",
            "Epoch 1 / 3, batch 141 / 469: loss: 2.296\n",
            "Epoch 1 / 3, batch 142 / 469: loss: 2.331\n",
            "Epoch 1 / 3, batch 143 / 469: loss: 2.321\n",
            "Epoch 1 / 3, batch 144 / 469: loss: 2.321\n",
            "Epoch 1 / 3, batch 145 / 469: loss: 2.326\n",
            "Epoch 1 / 3, batch 146 / 469: loss: 2.326\n",
            "Epoch 1 / 3, batch 147 / 469: loss: 2.315\n",
            "Epoch 1 / 3, batch 148 / 469: loss: 2.311\n",
            "Epoch 1 / 3, batch 149 / 469: loss: 2.297\n",
            "Epoch 1 / 3, batch 150 / 469: loss: 2.311\n",
            "Epoch 1 / 3, batch 151 / 469: loss: 2.325\n",
            "Epoch 1 / 3, batch 152 / 469: loss: 2.283\n",
            "Epoch 1 / 3, batch 153 / 469: loss: 2.328\n",
            "Epoch 1 / 3, batch 154 / 469: loss: 2.301\n",
            "Epoch 1 / 3, batch 155 / 469: loss: 2.320\n",
            "Epoch 1 / 3, batch 156 / 469: loss: 2.317\n",
            "Epoch 1 / 3, batch 157 / 469: loss: 2.314\n",
            "Epoch 1 / 3, batch 158 / 469: loss: 2.281\n",
            "Epoch 1 / 3, batch 159 / 469: loss: 2.331\n",
            "Epoch 1 / 3, batch 160 / 469: loss: 2.320\n",
            "Epoch 1 / 3, batch 161 / 469: loss: 2.335\n",
            "Epoch 1 / 3, batch 162 / 469: loss: 2.305\n",
            "Epoch 1 / 3, batch 163 / 469: loss: 2.301\n",
            "Epoch 1 / 3, batch 164 / 469: loss: 2.313\n",
            "Epoch 1 / 3, batch 165 / 469: loss: 2.300\n",
            "Epoch 1 / 3, batch 166 / 469: loss: 2.310\n",
            "Epoch 1 / 3, batch 167 / 469: loss: 2.313\n",
            "Epoch 1 / 3, batch 168 / 469: loss: 2.297\n",
            "Epoch 1 / 3, batch 169 / 469: loss: 2.298\n",
            "Epoch 1 / 3, batch 170 / 469: loss: 2.299\n",
            "Epoch 1 / 3, batch 171 / 469: loss: 2.311\n",
            "Epoch 1 / 3, batch 172 / 469: loss: 2.306\n",
            "Epoch 1 / 3, batch 173 / 469: loss: 2.308\n",
            "Epoch 1 / 3, batch 174 / 469: loss: 2.288\n",
            "Epoch 1 / 3, batch 175 / 469: loss: 2.311\n",
            "Epoch 1 / 3, batch 176 / 469: loss: 2.301\n",
            "Epoch 1 / 3, batch 177 / 469: loss: 2.293\n",
            "Epoch 1 / 3, batch 178 / 469: loss: 2.286\n",
            "Epoch 1 / 3, batch 179 / 469: loss: 2.315\n",
            "Epoch 1 / 3, batch 180 / 469: loss: 2.329\n",
            "Epoch 1 / 3, batch 181 / 469: loss: 2.301\n",
            "Epoch 1 / 3, batch 182 / 469: loss: 2.292\n",
            "Epoch 1 / 3, batch 183 / 469: loss: 2.334\n",
            "Epoch 1 / 3, batch 184 / 469: loss: 2.305\n",
            "Epoch 1 / 3, batch 185 / 469: loss: 2.306\n",
            "Epoch 1 / 3, batch 186 / 469: loss: 2.298\n",
            "Epoch 1 / 3, batch 187 / 469: loss: 2.304\n",
            "Epoch 1 / 3, batch 188 / 469: loss: 2.304\n",
            "Epoch 1 / 3, batch 189 / 469: loss: 2.317\n",
            "Epoch 1 / 3, batch 190 / 469: loss: 2.312\n",
            "Epoch 1 / 3, batch 191 / 469: loss: 2.307\n",
            "Epoch 1 / 3, batch 192 / 469: loss: 2.322\n",
            "Epoch 1 / 3, batch 193 / 469: loss: 2.308\n",
            "Epoch 1 / 3, batch 194 / 469: loss: 2.297\n",
            "Epoch 1 / 3, batch 195 / 469: loss: 2.315\n",
            "Epoch 1 / 3, batch 196 / 469: loss: 2.308\n",
            "Epoch 1 / 3, batch 197 / 469: loss: 2.309\n",
            "Epoch 1 / 3, batch 198 / 469: loss: 2.305\n",
            "Epoch 1 / 3, batch 199 / 469: loss: 2.310\n",
            "Epoch 1 / 3, batch 200 / 469: loss: 2.308\n",
            "Epoch 1 / 3, batch 201 / 469: loss: 2.310\n",
            "Epoch 1 / 3, batch 202 / 469: loss: 2.314\n",
            "Epoch 1 / 3, batch 203 / 469: loss: 2.315\n",
            "Epoch 1 / 3, batch 204 / 469: loss: 2.315\n",
            "Epoch 1 / 3, batch 205 / 469: loss: 2.292\n",
            "Epoch 1 / 3, batch 206 / 469: loss: 2.303\n",
            "Epoch 1 / 3, batch 207 / 469: loss: 2.292\n",
            "Epoch 1 / 3, batch 208 / 469: loss: 2.307\n",
            "Epoch 1 / 3, batch 209 / 469: loss: 2.318\n",
            "Epoch 1 / 3, batch 210 / 469: loss: 2.318\n",
            "Epoch 1 / 3, batch 211 / 469: loss: 2.300\n",
            "Epoch 1 / 3, batch 212 / 469: loss: 2.341\n",
            "Epoch 1 / 3, batch 213 / 469: loss: 2.323\n",
            "Epoch 1 / 3, batch 214 / 469: loss: 2.310\n",
            "Epoch 1 / 3, batch 215 / 469: loss: 2.317\n",
            "Epoch 1 / 3, batch 216 / 469: loss: 2.311\n",
            "Epoch 1 / 3, batch 217 / 469: loss: 2.287\n",
            "Epoch 1 / 3, batch 218 / 469: loss: 2.311\n",
            "Epoch 1 / 3, batch 219 / 469: loss: 2.304\n",
            "Epoch 1 / 3, batch 220 / 469: loss: 2.304\n",
            "Epoch 1 / 3, batch 221 / 469: loss: 2.307\n",
            "Epoch 1 / 3, batch 222 / 469: loss: 2.296\n",
            "Epoch 1 / 3, batch 223 / 469: loss: 2.314\n",
            "Epoch 1 / 3, batch 224 / 469: loss: 2.327\n",
            "Epoch 1 / 3, batch 225 / 469: loss: 2.290\n",
            "Epoch 1 / 3, batch 226 / 469: loss: 2.314\n",
            "Epoch 1 / 3, batch 227 / 469: loss: 2.309\n",
            "Epoch 1 / 3, batch 228 / 469: loss: 2.325\n",
            "Epoch 1 / 3, batch 229 / 469: loss: 2.308\n",
            "Epoch 1 / 3, batch 230 / 469: loss: 2.301\n",
            "Epoch 1 / 3, batch 231 / 469: loss: 2.314\n",
            "Epoch 1 / 3, batch 232 / 469: loss: 2.315\n",
            "Epoch 1 / 3, batch 233 / 469: loss: 2.310\n",
            "Epoch 1 / 3, batch 234 / 469: loss: 2.297\n",
            "Epoch 1 / 3, batch 235 / 469: loss: 2.300\n",
            "Epoch 1 / 3, batch 236 / 469: loss: 2.303\n",
            "Epoch 1 / 3, batch 237 / 469: loss: 2.308\n",
            "Epoch 1 / 3, batch 238 / 469: loss: 2.312\n",
            "Epoch 1 / 3, batch 239 / 469: loss: 2.336\n",
            "Epoch 1 / 3, batch 240 / 469: loss: 2.309\n",
            "Epoch 1 / 3, batch 241 / 469: loss: 2.323\n",
            "Epoch 1 / 3, batch 242 / 469: loss: 2.305\n",
            "Epoch 1 / 3, batch 243 / 469: loss: 2.298\n",
            "Epoch 1 / 3, batch 244 / 469: loss: 2.311\n",
            "Epoch 1 / 3, batch 245 / 469: loss: 2.312\n",
            "Epoch 1 / 3, batch 246 / 469: loss: 2.302\n",
            "Epoch 1 / 3, batch 247 / 469: loss: 2.304\n",
            "Epoch 1 / 3, batch 248 / 469: loss: 2.295\n",
            "Epoch 1 / 3, batch 249 / 469: loss: 2.307\n",
            "Epoch 1 / 3, batch 250 / 469: loss: 2.305\n",
            "Epoch 1 / 3, batch 251 / 469: loss: 2.310\n",
            "Epoch 1 / 3, batch 252 / 469: loss: 2.311\n",
            "Epoch 1 / 3, batch 253 / 469: loss: 2.302\n",
            "Epoch 1 / 3, batch 254 / 469: loss: 2.316\n",
            "Epoch 1 / 3, batch 255 / 469: loss: 2.307\n",
            "Epoch 1 / 3, batch 256 / 469: loss: 2.329\n",
            "Epoch 1 / 3, batch 257 / 469: loss: 2.320\n",
            "Epoch 1 / 3, batch 258 / 469: loss: 2.304\n",
            "Epoch 1 / 3, batch 259 / 469: loss: 2.311\n",
            "Epoch 1 / 3, batch 260 / 469: loss: 2.302\n",
            "Epoch 1 / 3, batch 261 / 469: loss: 2.304\n",
            "Epoch 1 / 3, batch 262 / 469: loss: 2.309\n",
            "Epoch 1 / 3, batch 263 / 469: loss: 2.300\n",
            "Epoch 1 / 3, batch 264 / 469: loss: 2.303\n",
            "Epoch 1 / 3, batch 265 / 469: loss: 2.302\n",
            "Epoch 1 / 3, batch 266 / 469: loss: 2.303\n",
            "Epoch 1 / 3, batch 267 / 469: loss: 2.304\n",
            "Epoch 1 / 3, batch 268 / 469: loss: 2.307\n",
            "Epoch 1 / 3, batch 269 / 469: loss: 2.319\n",
            "Epoch 1 / 3, batch 270 / 469: loss: 2.298\n",
            "Epoch 1 / 3, batch 271 / 469: loss: 2.291\n",
            "Epoch 1 / 3, batch 272 / 469: loss: 2.288\n",
            "Epoch 1 / 3, batch 273 / 469: loss: 2.356\n",
            "Epoch 1 / 3, batch 274 / 469: loss: 2.309\n",
            "Epoch 1 / 3, batch 275 / 469: loss: 2.306\n",
            "Epoch 1 / 3, batch 276 / 469: loss: 2.311\n",
            "Epoch 1 / 3, batch 277 / 469: loss: 2.310\n",
            "Epoch 1 / 3, batch 278 / 469: loss: 2.289\n",
            "Epoch 1 / 3, batch 279 / 469: loss: 2.297\n",
            "Epoch 1 / 3, batch 280 / 469: loss: 2.336\n",
            "Epoch 1 / 3, batch 281 / 469: loss: 2.308\n",
            "Epoch 1 / 3, batch 282 / 469: loss: 2.317\n",
            "Epoch 1 / 3, batch 283 / 469: loss: 2.299\n",
            "Epoch 1 / 3, batch 284 / 469: loss: 2.307\n",
            "Epoch 1 / 3, batch 285 / 469: loss: 2.289\n",
            "Epoch 1 / 3, batch 286 / 469: loss: 2.288\n",
            "Epoch 1 / 3, batch 287 / 469: loss: 2.289\n",
            "Epoch 1 / 3, batch 288 / 469: loss: 2.291\n",
            "Epoch 1 / 3, batch 289 / 469: loss: 2.306\n",
            "Epoch 1 / 3, batch 290 / 469: loss: 2.290\n",
            "Epoch 1 / 3, batch 291 / 469: loss: 2.288\n",
            "Epoch 1 / 3, batch 292 / 469: loss: 2.302\n",
            "Epoch 1 / 3, batch 293 / 469: loss: 2.283\n",
            "Epoch 1 / 3, batch 294 / 469: loss: 2.242\n",
            "Epoch 1 / 3, batch 295 / 469: loss: 2.261\n",
            "Epoch 1 / 3, batch 296 / 469: loss: 2.224\n",
            "Epoch 1 / 3, batch 297 / 469: loss: 2.218\n",
            "Epoch 1 / 3, batch 298 / 469: loss: 2.264\n",
            "Epoch 1 / 3, batch 299 / 469: loss: 2.226\n",
            "Epoch 1 / 3, batch 300 / 469: loss: 2.165\n",
            "Epoch 1 / 3, batch 301 / 469: loss: 2.270\n",
            "Epoch 1 / 3, batch 302 / 469: loss: 2.093\n",
            "Epoch 1 / 3, batch 303 / 469: loss: 2.098\n",
            "Epoch 1 / 3, batch 304 / 469: loss: 2.120\n"
          ]
        }
      ],
      "source": [
        "# Start training.\n",
        "trainer.run(train_loader, max_epochs=num_epochs)"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}